{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5605bc8",
   "metadata": {},
   "source": [
    "# Peace-Sign Selfie (Colab-compatible)\n",
    "\n",
    "This notebook opens the webcam (in Colab or a local Jupyter runtime), uses Mediapipe to detect a ✌️ peace sign (index + middle fingers up), shows a 2-second countdown, takes a selfie, saves it as `selfie_YYYYMMDD_HHMMSS.jpg`, applies a simple filter, and displays the result.\n",
    "\n",
    "Requirements: Python, OpenCV, Mediapipe. Install commands are below. If running in Colab, you may need to accept camera permissions in the browser. If you run this in a local Jupyter (not Colab), the normal OpenCV camera code will be used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958632ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run this cell first)\n",
    "!pip install --quiet mediapipe opencv-python-headless==4.7.0.72 numpy\n",
    "# opencv-python-headless is used in Colab; if running locally and you need GUI windows, install opencv-python instead:\n",
    "# !pip install opencv-python mediapipe numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc2c5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and helper functions\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "from base64 import b64decode\n",
    "\n",
    "# Colab-specific helpers (use eval_js to access browser camera stream)\n",
    "try:\n",
    "    from google.colab.output import eval_js\n",
    "    from google.colab.patches import cv2_imshow\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Finger landmark indexes (Mediapipe)\n",
    "TIP_IDS = [4, 8, 12, 16, 20]  # thumb, index, middle, ring, pinky\n",
    "\n",
    "def is_finger_up(hand_landmarks, finger_tip_idx, finger_pip_idx):\n",
    "    # hand_landmarks: mediapipe normalized landmarks list; returns True if finger appears up.\n",
    "    # In image coords, y increases downwards. A finger is \n",
    " if tip y < pip y.\n",
    "    return hand_landmarks.landmark[finger_tip_idx].y < hand_landmarks.landmark[finger_pip_idx].y\n",
    "\n",
    "def detect_peace_sign(hand_landmarks):\n",
    "    # Returns True if index and middle fingers are up, ring and pinky are down. Thumb ignored for simplicity.\n",
    "    # finger tip and pip indices for mediapipe: index tip=8, pip=6; middle tip=12, pip=10; ring tip=16, pip=14; pinky tip=20, pip=18\n",
    "    try:\n",
    "        idx_up = is_finger_up(hand_landmarks, 8, 6)\n",
    "        mid_up = is_finger_up(hand_landmarks, 12, 10)\n",
    "        ring_up = is_finger_up(hand_landmarks, 16, 14)\n",
    "        pinky_up = is_finger_up(hand_landmarks, 20, 18)\n",
    "        # Peace sign: index & middle up; ring & pinky down.\n",
    "        return idx_up and mid_up and (not ring_up) and (not pinky_up)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def apply_filter(img, mode='sepia'):\n",
    "    # img in BGR (OpenCV)\n",
    "    if mode == 'grayscale':\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        return cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)\n",
    "    elif mode == 'sepia':\n",
    "        # simple sepia kernel\n",
    "        kernel = np.array([[0.272, 0.534, 0.131],\n",
    "                           [0.349, 0.686, 0.168],\n",
    "                           [0.393, 0.769, 0.189]])\n",
    "        img_sepia = cv2.transform(img, kernel)\n",
    "        img_sepia = np.clip(img_sepia, 0, 255).astype(np.uint8)\n",
    "        return img_sepia\n",
    "    elif mode == 'cartoon':\n",
    "        # cartoon effect: bilateral filter + edge mask\n",
    "        img_color = cv2.bilateralFilter(img, d=9, sigmaColor=75, sigmaSpace=75)\n",
    "        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img_blur = cv2.medianBlur(img_gray, 7)\n",
    "        edges = cv2.adaptiveThreshold(img_blur, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 9, 2)\n",
    "        edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)\n",
    "        cartoon = cv2.bitwise_and(img_color, edges_colored)\n",
    "        return cartoon\n",
    "    else:\n",
    "        return img\n",
    "\n",
    "def timestamp_filename(basename='selfie', ext='jpg'):\n",
    "    t = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    return f\"{basename}_{t}.{ext}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b66a22",
   "metadata": {},
   "source": [
    "## Colab camera helper (if running in Google Colab)\n",
    "\n",
    "The following starts the browser camera once and lets Python fetch frames repeatedly using `get_frame()`. If you're running this notebook in a local Jupyter environment with direct OpenCV access, you can skip this cell and use the local camera code in the main loop instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3b4d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    print('Running in Colab environment. Using browser camera streaming helper.')\n",
    "    def start_camera(width=640, height=480):\n",
    "        # Initializes browser camera and stores references to a video element and canvas in window._video/_canvas\n",
    "        js = f\"\n",
    "        async function startCamera(){\n",
    "          const video = document.createElement('video');\n",
    "          video.style.display = 'block';\n",
    "          video.width = {width};\n",
    "          video.height = {height};\n",
    "          document.body.appendChild(video);\n",
    "          const stream = await navigator.mediaDevices.getUserMedia({ video: true });\n",
    "          video.srcObject = stream;\n",
    "          await video.play();\n",
    "          const canvas = document.createElement('canvas');\n",
    "          canvas.width = {width};\n",
    "          canvas.height = {height};\n",
    "          window._video = video;\n",
    "          window._canvas = canvas;\n",
    "          return true;\n",
    "        }\n",
    "        startCamera();\n",
    "        \"\n",
    "        return eval_js(js)\n",
    "\n",
    "    def get_frame():\n",
    "        # Returns an OpenCV BGR image captured from the browser video element.\n",
    "        data = eval_js(\n",
    "\n",
    ",\n",
    ",\n",
    "\n",
    ")\n",
    "        header, encoded = data.split(',', 1)\n",
    "        img_bytes = b64decode(encoded)\n",
    "        nparr = np.frombuffer(img_bytes, np.uint8)\n",
    "        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "        return img\n",
    "else:\n",
    "    print('Not running in Colab. Will try to use local camera via OpenCV VideoCapture.')\n",
    "\n",
    "# End of camera helper cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b857779",
   "metadata": {},
   "source": [
    "## Main loop: detect gesture and take selfie\n",
    "\n",
    "Run this cell to start the live detection. In Colab, first run the start_camera cell and allow camera permissions. Then run this cell. Use Ctrl+C in a local runtime to stop, or interrupt the kernel in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f3ae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop implementation\n",
    "SAVE_DIR = '/content' if IN_COLAB else os.getcwd()\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "FILTER = 'sepia'  # choose 'grayscale', 'sepia', or 'cartoon'\n",
    "DELAY_SECONDS = 2  # delay before taking selfie after gesture detected\n",
    "\n",
    "if IN_COLAB:\n",
    "    start_camera(640, 480)\n",
    "    time.sleep(1.0)\n",
    "\n",
    "cap = None\n",
    "if not IN_COLAB:\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError('Could not open camera. Is camera available?')\n",
    "\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.6, min_tracking_confidence=0.5)\n",
    "gesture_active = False\n",
    "gesture_start = None\n",
    "captured = False\n",
    "last_frame = None\n",
    "\n",
    "print('Starting detection. Show a ✌️ peace sign to trigger a selfie.')\n",
    "try:\n",
    "    while True:\n",
    "        if IN_COLAB:\n",
    "            frame = get_frame()  # BGR image\n",
    "        else:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print('Failed to grab frame')\n",
    "                break\n",
    "        last_frame = frame.copy()\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(image_rgb)\n",
    "\n",
    "        h, w, _ = frame.shape\n",
    "        display_frame = frame.copy()\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(display_frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                if detect_peace_sign(hand_landmarks):\n",
    "                    if not gesture_active:\n",
    "                        gesture_active = True\n",
    "                        gesture_start = time.time()\n",
    "                        captured = False\n",
    "                    # compute remaining countdown\n",
    "                    elapsed = time.time() - gesture_start\n",
    "                    remaining = max(0, int(DELAY_SECONDS - elapsed) + 1)\n",
    "                    text = f'Taking Selfie in {max(0, round(DELAY_SECONDS - elapsed,1))}s'\n",
    "                    cv2.putText(display_frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0,255,0), 2, cv2.LINE_AA)\n",
    "                    # draw small countdown number\n",
    "                    cv2.putText(display_frame, str(remaining), (w-80, 80), cv2.FONT_HERSHEY_DUPLEX, 3.0, (0,0,255), 6)\n",
    "                    if elapsed >= DELAY_SECONDS and not captured:\n",
    "                        # take selfie\n",
    "                        filename = timestamp_filename('selfie', 'jpg')\n",
    "                        full_path = os.path.join(SAVE_DIR, filename)\n",
    "                        cv2.imwrite(full_path, last_frame)\n",
    "                        print(f'Saved selfie to: {full_path}')\n",
    "                        # apply filter and display result\n",
    "                        filtered = apply_filter(last_frame, mode=FILTER)\n",
    "                        out_name = os.path.join(SAVE_DIR, 'filtered_' + filename)\n",
    "                        cv2.imwrite(out_name, filtered)\n",
    "                        print(f'Saved filtered selfie to: {out_name}')\n",
    "                        # optional sound: in Colab we can show a small JS beep (or just print)\n",
    "                        try:\n",
    "                            if IN_COLAB:\n",
    "                                # play a short beep using JS audio (embedded)\n",
    "                                eval_js("
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
